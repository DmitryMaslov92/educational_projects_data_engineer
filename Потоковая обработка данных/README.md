# Потоковая обработка данных

## Описание:

В учебном проекте, предшествующем данной итоговой работе, если условия рекламных кампаний и местоположение пользователя совпадали по заданным признакам, сервис отправлял сообщение в сервис push-уведомлений, а уже этот сервис push-уведомлений — уведомления пользователям. 

Сейчас нужно создать приложение, которое будет сужать круг пользователей ещё больше и доставлять уведомления об акциях с ограниченным сроком действия. 

## Цель работы:

Создать сервис push-уведомлений для пользователей приложения, направленный на получение данных об акциях ресторана. 

Сервис должен будет:
 - читать данные из Kafka с помощью Spark Structured Streaming и Python в режиме реального времени;
 - получать список подписчиков из базы данных Postgres;
 - джойнить данные из Kafka с данными из БД;
 - сохранять в памяти полученные данные, чтобы не собирать их заново после отправки в Postgres или Kafka;
 - отправлять выходное сообщение в Kafka с информацией об акции, пользователе со списком избранного и ресторане;
 - вставлять записи в Postgres, чтобы впоследствии получить фидбэк от пользователя.
 
 Сервис push-уведомлений должен будет читать сообщения из Kafka и формировать готовые уведомления.

## Этапы работы: 

1. Проверить работу потока.
2. Прочитать данные об акциях из Kafka.
3. Прочитать данные о подписчиках из Postgres.
4. Преобразование JSON в датафейм.
5. Провести JOIN потоковых и статичных данных.
6. Отправить результаты JOIN в Postgres для аналитики фидбэка.
7. Отправить данные, сериализованные в формат JSON, в Kafka для push-уведомлений.
8. Обеспечить персистентность датафрейма.

## Описание рабочих файлов:

Папка src/scripts. 

project_8.py - файл с итоговым проектом.

project_config.ini - файл с параметрами подключения к Kafka.

